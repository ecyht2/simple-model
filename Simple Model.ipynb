{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a3f895-4eef-4e0b-8029-bf6e5f1a89f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -U -q transformers torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1840afc-d270-4c8d-abc7-20df11a35183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "batch_size = 5\n",
    "kwargs = {'batch_size': batch_size}\n",
    "\n",
    "dataset1 = MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "dataset2 = MNIST(\"./data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "\n",
    "plt.imshow(dataset1[0][0].squeeze())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6240aaf-1709-49d8-822c-7eab069e3cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from simple_model.pytorch_model import Model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net = Model()\n",
    "net = net.to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727933b-cccb-47bc-ae41-0f5e2cc15059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import zeros\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "net.train()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"EPOC {epoch + 1}/{EPOCH}\")\n",
    "    for data, target in tqdm(train_loader):\n",
    "        data, target_tensor = data.to(device), zeros(target.shape[0], 10).to(device)\n",
    "        for batch_id, idx in enumerate(target):\n",
    "            target_tensor[batch_id, idx] = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.cross_entropy(output, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99d17f-248b-4074-a3ec-68f1a246db16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader):\n",
    "        target = target.to(device)\n",
    "        data, target_tensor = data.to(device), zeros(target.shape[0], 10).to(device)\n",
    "        for batch_id, idx in enumerate(target):\n",
    "            target_tensor[batch_id, idx] = 1.0\n",
    "        output = net(data)\n",
    "        test_loss += F.cross_entropy(output, target_tensor, reduction='sum').item()  # sum up batch loss\n",
    "        pred = net.get_guess(output)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "torch.save(net.state_dict(), \"simple_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1f31c-dfae-4f04-b80f-147aab240be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "data = MNIST('./data')\n",
    "test = [data[0][0], data[1][0]]\n",
    "\n",
    "display(test[0])\n",
    "display(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce497216-b547-43c7-b521-9340dbac6277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simple_model import SimpleModel, SimpleModelConfig\n",
    "\n",
    "SimpleModelConfig.register_for_auto_class(\"AutoConfig\")\n",
    "SimpleModel.register_for_auto_class(\"AutoModel\")\n",
    "SimpleModel.register_for_auto_class(\"AutoModelForImageClassification\")\n",
    "\n",
    "simple_model_config = SimpleModelConfig()\n",
    "simple_model = SimpleModel(simple_model_config)\n",
    "simple_model.model.load_state_dict(net.state_dict())\n",
    "\n",
    "simple_model.save_pretrained(\"simple-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6283d6-0599-4ab8-bd16-00f625f1fd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simple_model import SimpleModelProcessor\n",
    "\n",
    "SimpleModelProcessor.register_for_auto_class(\"AutoProcessor\")\n",
    "SimpleModelProcessor.register_for_auto_class(\"AutoImageProcessor\")\n",
    "\n",
    "processor = SimpleModelProcessor()\n",
    "processor.save_pretrained(\"simple-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834bbc22-fdf9-4823-9990-6187d2e6239f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_input = processor(test)\n",
    "    test_output = simple_model(**test_input)\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17609b00-b7da-41e4-967a-e52ec6de163b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-classification\",\n",
    "    model=simple_model,\n",
    "    image_processor=processor,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "pipe.save_pretrained(\"simple-model\")\n",
    "pipe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec808c94-dddc-49dc-94dc-0eface7024ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoProcessor\n",
    "\n",
    "output_processor = AutoProcessor.from_pretrained(\n",
    "    \"simple-model\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "output_model = AutoModel.from_pretrained(\n",
    "    \"simple-model\",\n",
    "    # Loading config manually as it is registered in library\n",
    "    config=AutoConfig.from_pretrained(\"simple-model\"),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_input = processor(test)\n",
    "    test_output = output_model(**test_input)\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb4a22-0c15-4501-aae2-8ad60c80ce9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "output_pipeline = pipeline(\n",
    "    \"image-classification\",\n",
    "    model=\"simple-model\",\n",
    "    image_processor=\"simple-model\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "output_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b82923-3633-4586-a8ec-f96ca02066ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -U -q \"optimum[exporters]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3271c-5e32-4d64-a678-a2b94bd19913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import simple_model\n",
    "\n",
    "reload(simple_model)\n",
    "reload(simple_model.configuration_simple_model)\n",
    "reload(simple_model.modeling_simple_model)\n",
    "reload(simple_model.processing_simple_model)\n",
    "reload(simple_model.pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b80d47-578a-4a81-9093-2031a94508d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from optimum.exporters import TasksManager\n",
    "from optimum.exporters.onnx import export\n",
    "from transformers import AutoConfig, AutoModel, AutoProcessor\n",
    "\n",
    "from simple_model.configuration_simple_model import SimpleModelOnnxConfig\n",
    "\n",
    "output_config = AutoConfig.from_pretrained(\"simple-model\")\n",
    "output_model = AutoModel.from_pretrained(\n",
    "    \"simple-model\",\n",
    "    config=output_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "onnx_config = SimpleModelOnnxConfig(output_config, task=\"image-classification\")\n",
    "print(f\"ONNX Inputs: {onnx_config.inputs}\")\n",
    "print(f\"ONNX Outputs: {onnx_config.outputs}\")\n",
    "print(f\"ONNX Opset: {onnx_config.DEFAULT_ONNX_OPSET}\")\n",
    "\n",
    "TasksManager._SUPPORTED_MODEL_TYPE[\"simple-model\"] = {\n",
    "    \"onnx\": {\n",
    "        \"image-classification\": SimpleModelOnnxConfig\n",
    "    }\n",
    "}\n",
    "\n",
    "onnx_config_constructor = TasksManager.get_exporter_config_constructor(\n",
    "    \"onnx\",\n",
    "    output_model,\n",
    "    task=\"image-classification\"\n",
    ")\n",
    "onnx_config = onnx_config_constructor(output_model.config, task=\"image-classification\")\n",
    "\n",
    "onnx_inputs, onnx_outputs = export(\n",
    "    output_model,\n",
    "    onnx_config,\n",
    "    Path(\"simple-model.onnx\"),\n",
    "    onnx_config.DEFAULT_ONNX_OPSET,\n",
    "    input_shapes={\n",
    "        \"num_channels\": 1,\n",
    "        \"height\": 28,\n",
    "        \"width\": 28\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
